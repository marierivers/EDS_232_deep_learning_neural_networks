---
title: "deep_learning_neural_networks_R"
author: "Marie Rivers"
date: "2/13/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#load libraries
librarian::shelf(
  devtools,
  keras,
  reticulate,
  tensorflow)
```

```{r}
# show library versions and paths
session_info()
```

```{r}
# install Python into user space
(reticulate::miniconda_path()) # show the Python path
if(!file.exists(reticulate::miniconda_path()))
  reticulate::install_miniconda()

# install keras with tensorflow
if(!keras::is_keras_available())
  keras::install_keras()
```

# Loading the MNIST dataset in Keras
```{r}
mnist <- dataset_mnist()
```

```{r}
train_images <- mnist$train$x
train_labels <- mnist$train$y
test_images <- mnist$test$x
test_labels <- mnist$test$y
```

The R `str()` function is a convenient way to get a quick glimpse at the structure of an array.
```{r}
str(train_images)
```

```{r}
str(train_labels)
```

```{r}
str(test_images)
```

```{r}
str(test_labels)
```

The workflow: First, feed the neural network the training data, `train_images` and `train_labels`. The network will then learn to associate images and labels. Finally, ask the network to produce predictions for `test_images`, and verify whether these predictions match the labels from `test_labels`.

# The network architecture
```{r}
network <- keras_model_sequential() %>% 
  layer_dense(units = 512, activation = "relu", input_shape = c(28 * 28)) %>% 
  layer_dense(units = 10, activation = "softmax")
```
Loss function - how the network will be able to measure how good a job it's doing on its training data, and thus how it will be able to steer itself in the right direction.

Optimizer - the mechanism through which the network will update itself based on the data it sees and its loss function.

accuracy - the fraction of the images that were correctly classified

# The compilation step
```{r}
network %>% compile(
  optimizer = "rmsprop",
  loss = "categorical_crossentropy",
  metrics = c("accuracy"))
```

Before training, preprocess the data by reshaping it into the shape the network expects and scaling it so that all values are in the [0, 1] interval. Previously, the training images were stored in an array of shape (60000, 28, 28) or type integer with values in the [0, 255] interval. Transform it into a double array of shape (60000, 28 * 28) with values between 0 and 1.

# Preparing the image data
```{r}
train_images <- array_reshape(train_images, c(60000, 28 * 28))
train_images <- train_images / 255
test_images <- array_reshape(test_images, c(10000, 28 * 28))
test_images <- test_images / 255
```

# Preparing the labels
categorically encode the labels
```{r}
train_labels <- to_categorical(train_labels)
test_labels <- to_categorical(test_labels)
```

# "Fit" the model to the training data
```{r}
network %>% fit(train_images, train_labels, epochs = 5, batch_size = 128)
```

# Check how the model performs on the test set
```{r}
metrics <- network %>% evaluate(test_images, test_labels, verbose = 0)
metrics
```

The gap between the training accuracy and test accuracy is an example of "overfitting" and the fact that machine learning models tend to perform worse on new data than on their training data.